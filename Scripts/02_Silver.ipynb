{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541b8280-e4d9-405a-9ae1-da76fd5307b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit, sha2, concat_ws, coalesce, current_timestamp\n",
    "\n",
    "# ------------------------------\n",
    "# Table Configurations\n",
    "# ------------------------------\n",
    "bronze_table = \"bronze_dev.bronze_dev.stg_bronze_superstore\"\n",
    "silver_fact_table = \"silver_dev.silver_dev.silver_superstore\"\n",
    "silver_customers_table = \"silver_dev.silver_dev.dim_customers\"\n",
    "silver_products_table = \"silver_dev.silver_dev.dim_products\"\n",
    "silver_date_table = \"silver_dev.silver_dev.dim_date\"\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Read Bronze Table\n",
    "# ------------------------------\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Data Quality: Filter negative quantities and standardize dates\n",
    "# ------------------------------\n",
    "df_silver = df_bronze.filter(col(\"quantity\") > 0) \\\n",
    "                     .withColumn(\"order_date\", to_timestamp(\"order_date\", \"MM/dd/yyyy\")) \\\n",
    "                     .withColumn(\"ship_date\", to_timestamp(\"ship_date\", \"MM/dd/yyyy\")) \\\n",
    "                     .withColumn(\"city\", coalesce(col(\"city\"), lit(\"UNKNOWN\"))) \\\n",
    "                     .withColumn(\"postal_code\", coalesce(col(\"postal_code\"), lit(\"00000\")).cast(\"int\") \\\n",
    "                     .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "                     .withColumn(\"sales\", col(\"sales\").cast(\"double\")) \\\n",
    "                     .withColumn(\"discount\", col(\"discount\").cast(\"double\")) \\\n",
    "                     .withColumn(\"profit\", col(\"profit\").cast(\"double\")))\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Rename Columns to Business-Friendly Names\n",
    "# ------------------------------\n",
    "df_silver = df_silver.withColumnRenamed(\"category\", \"product_category\") \\\n",
    "                     .withColumnRenamed(\"sub_category\", \"product_sub_category\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create Record Hash for Idempotency\n",
    "# ------------------------------\n",
    "business_cols = [c for c in df_silver.columns if c not in [\"_metadata\", \"_ingest_ts\"]]\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"_record_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in business_cols]), 256)\n",
    ").withColumn(\"_ingest_ts\", current_timestamp())\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Maintain Dimension Tables\n",
    "# ------------------------------\n",
    "\n",
    "# Customers Dimension\n",
    "dim_customers = df_silver.select(\"customer_name\", \"city\", \"postal_code\").distinct() \\\n",
    "                          .withColumn(\"customer_id\", sha2(col(\"customer_name\"), 256))\n",
    "\n",
    "if spark.catalog.tableExists(silver_customers_table):\n",
    "    existing_customers = spark.table(silver_customers_table).select(\"customer_id\")\n",
    "    new_customers = dim_customers.join(existing_customers, on=\"customer_id\", how=\"left_anti\")\n",
    "    if new_customers.count() > 0:\n",
    "        new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(silver_customers_table)\n",
    "else:\n",
    "    dim_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_customers_table)\n",
    "\n",
    "# Products Dimension\n",
    "dim_products = df_silver.select(\"product_id\", \"product_category\", \"product_sub_category\").distinct() \\\n",
    "                        .withColumn(\"product_key\", sha2(col(\"product_id\"), 256))\n",
    "\n",
    "if spark.catalog.tableExists(silver_products_table):\n",
    "    existing_products = spark.table(silver_products_table).select(\"product_key\")\n",
    "    new_products = dim_products.join(existing_products, on=\"product_key\", how=\"left_anti\")\n",
    "    if new_products.count() > 0:\n",
    "        new_products.write.format(\"delta\").mode(\"append\").saveAsTable(silver_products_table)\n",
    "else:\n",
    "    dim_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_products_table)\n",
    "\n",
    "# Date Dimension\n",
    "dim_date = df_silver.select(\"order_date\").distinct() \\\n",
    "                    .withColumn(\"order_date_key\", sha2(col(\"order_date\").cast(\"string\"), 256))\n",
    "\n",
    "if spark.catalog.tableExists(silver_date_table):\n",
    "    existing_date = spark.table(silver_date_table).select(\"order_date_key\")\n",
    "    new_dates = dim_date.join(existing_date, on=\"order_date_key\", how=\"left_anti\")\n",
    "    if new_dates.count() > 0:\n",
    "        new_dates.write.format(\"delta\").mode(\"append\").saveAsTable(silver_date_table)\n",
    "else:\n",
    "    dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_date_table)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Idempotent Load for Silver Fact Table\n",
    "# ------------------------------\n",
    "if spark.catalog.tableExists(silver_fact_table):\n",
    "    df_existing = spark.table(silver_fact_table).select(\"_record_hash\")\n",
    "    df_new = df_silver.join(df_existing, on=\"_record_hash\", how=\"left_anti\")\n",
    "else:\n",
    "    df_new = df_silver\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Write New Rows Only\n",
    "# ------------------------------\n",
    "if df_new.count() > 0:\n",
    "    df_new.write.format(\"delta\").mode(\"append\").saveAsTable(silver_fact_table)\n",
    "    print(f\"Inserted {df_new.count()} new rows into Silver fact table.\")\n",
    "else:\n",
    "    print(\"No new rows to insert.\")\n",
    "\n",
    "print(\"Silver Layer processing completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c83e49-e122-4013-ab69-e45f138ff02d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765652250221}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f' select distinct quantity from bronze_dev.bronze_dev.stg_bronze_superstore').display()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
